{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbc1515-947b-466f-99fb-00ee29587e66",
   "metadata": {},
   "source": [
    "We'll use a Long Short-Term Memory (LSTM) network, which is well-suited for time series prediction tasks like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb0a267-8350-41f6-8ba2-0943f995146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7deb781c-be56-49d4-abf0-3b0ef7225bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(folder_path, sequence_length=12, prediction_horizon=6):\n",
    "    all_data = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            features = ['cbg', 'finger', 'basal', 'hr', 'gsr', 'carbInput', 'bolus']\n",
    "            data = df[features].values\n",
    "            \n",
    "            # Check for and remove any rows with NaN or infinite values\n",
    "            data = data[~np.isnan(data).any(axis=1)]\n",
    "            data = data[~np.isinf(data).any(axis=1)]\n",
    "            \n",
    "            all_data.append(data)\n",
    "    \n",
    "    combined_data = np.concatenate(all_data, axis=0)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))  # Change scale to (-1, 1)\n",
    "    data_scaled = scaler.fit_transform(combined_data)\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(data_scaled) - sequence_length - prediction_horizon):\n",
    "        X.append(data_scaled[i:(i + sequence_length)])\n",
    "        y.append(data_scaled[i + sequence_length + prediction_horizon, 0])\n",
    "    \n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh', input_shape=input_shape, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        LSTM(32, activation='tanh'),\n",
    "        BatchNormalization(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001, clipnorm=1.0), loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, epochs=20, batch_size=32):\n",
    "    model = create_model((X_train.shape[1], X_train.shape[2]))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_data=(X_val, y_val), verbose=1, callbacks=[early_stopping])\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2340048-135c-444c-a0d2-1e80706095a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BatchNormalization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_train_full, y_train_full, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(X_train, y_train, X_val, y_val, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m---> 43\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     46\u001b[0m                         validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(input_shape):\n\u001b[0;32m     31\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m     32\u001b[0m         LSTM(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39minput_shape, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m---> 33\u001b[0m         \u001b[43mBatchNormalization\u001b[49m(),\n\u001b[0;32m     34\u001b[0m         LSTM(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     35\u001b[0m         BatchNormalization(),\n\u001b[0;32m     36\u001b[0m         Dense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     37\u001b[0m         Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m     ])\n\u001b[0;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, clipnorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BatchNormalization' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "train_folder = r'C:\\Users\\alessia\\Documents\\00professional\\school\\04_unibe\\semester3\\diabetes_management\\Ohio Data\\Ohio2018_processed\\train'\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a18c78ff-366a-465f-ae8f-5a35625ad752",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BatchNormalization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(X_train, y_train, X_val, y_val, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m---> 43\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     46\u001b[0m                         validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(input_shape):\n\u001b[0;32m     31\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m     32\u001b[0m         LSTM(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39minput_shape, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m---> 33\u001b[0m         \u001b[43mBatchNormalization\u001b[49m(),\n\u001b[0;32m     34\u001b[0m         LSTM(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     35\u001b[0m         BatchNormalization(),\n\u001b[0;32m     36\u001b[0m         Dense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     37\u001b[0m         Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m     ])\n\u001b[0;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, clipnorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BatchNormalization' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677a4173-4d68-4a19-92cf-9e06c42a22d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "val_loss = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b81020-e1d8-40f9-b405-f74de524951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate finger stick samples\n",
    "def simulate_finger_stick_samples(X, n_samples):\n",
    "    mask = np.zeros_like(X)\n",
    "    mask[:, ::n_samples, 0] = 1  # Set every nth CGM value to 1\n",
    "    return X * mask\n",
    "\n",
    "# Experiment with different numbers of finger stick samples\n",
    "for n_samples in [4, 6, 8, 12]:\n",
    "    X_sampled = simulate_finger_stick_samples(X, n_samples)\n",
    "    X_train_sampled, X_val_sampled, _, _ = train_test_split(X_sampled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model_sampled, _ = train_model(X_train_sampled, y_train, X_val_sampled, y_val)\n",
    "    val_loss_sampled = model_sampled.evaluate(X_val_sampled, y_val)\n",
    "    print(f\"Validation Loss with {n_samples} finger stick samples: {val_loss_sampled}\")\n",
    "\n",
    "# Note: For testing, you would use a similar approach with the test folder\n",
    "# test_folder = 'data/test'\n",
    "# X_test, y_test, _ = load_and_preprocess_data(test_folder)\n",
    "# test_loss = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174194d-d013-49db-949f-d210bae3a070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
